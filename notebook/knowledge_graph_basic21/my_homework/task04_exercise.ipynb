{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task04 用户输入->知识库的查询语句\n",
    "## 1 知识梳理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 问答系统\n",
    "- 问答系统概念：用来回答用户提出的自然语言问题的系统\n",
    "- 类型：  \n",
    "从知识领域划分：封闭领域、开放领域  \n",
    "从实现方式划分：基于流水线（pipeline）实现、基于端到端（end-to-end）实现  \n",
    "从答案来源划分：知识库问答（KB-QA）、常见问题问答、行为问答、网际网路问答\n",
    "- 知识库问答流程：  \n",
    "问句->语义解析->语义表达->语义匹配、查询、推理<->知识库"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Query理解\n",
    "- 概念：从词法、句法、语义三个层面对Query进行结构化解析\n",
    "- 意图识别：检测用户当前输入的（文本/图片/语音）意图，所用方法包括基于词典模板的规则分类、传统的机器学习模型（文本特征工程+分类器）、深度学习模型\n",
    "- 槽值填充：通过既定的字段，将用户输入的信息与字段对应的部分提取出来\n",
    "- 序列标注的任务常用模型：词典匹配、BILSTM+CRF、IDCNN、BERT等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 基于知识图谱的问答系统框架"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/task04/01.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 代码详解\n",
    "主要详解主体类`EntityExtractor`代码（entity_extractor.py），主要用于命名实体识别、意图识别以及意图补充和纠正"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 命名实体识别\n",
    "整体思路：\n",
    "1. 根据用户的输入语句，使用预先构建的`AC Tree`(疾病、疾病别名、并发症和症状)进行匹配\n",
    "2. 如果匹配不到上述实体、使用`jieba`词库进行文本切分\n",
    "3. 将文本切分后的每一个词与词库（疾病、疾病别名、并发症和症状）中的词计算文本相似度得分（使用overlap score、余弦相似度和编辑距离计算平均得分），如果得分超过0.7，则该词属于相关类的实体\n",
    "4. 排序选取相关性得分最高的词作为实体"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph\n",
    "graph = Graph(\"http://localhost:7474\", username=\"neo4j\", password=\"hun1988\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Complication', 'Alias', 'Drug', 'Symptom', 'Part', 'Department', 'Disease']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "node_labes = list(graph.schema.node_labels)\n",
    "node_labes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "columns = ['EntityType', 'Numbers', 'Examples']\n",
    "df = pd.DataFrame(columns=columns)\n",
    "\n",
    "for label in node_labes:\n",
    "    node_matcher = graph.nodes.match(label)\n",
    "    count = node_matcher.count()\n",
    "    example = ', '.join([node['name'] for node in node_matcher.all()[:2]])\n",
    "    df = pd.concat([df, pd.DataFrame(data=[[label, count, example]], columns=columns)])\n",
    "    \n",
    "df = pd.concat([df, pd.DataFrame(data=[[\"Total\", df.Numbers.sum(), \"\"]], columns=columns)]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EntityType</th>\n",
       "      <th>Numbers</th>\n",
       "      <th>Examples</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Complication</td>\n",
       "      <td>3201</td>\n",
       "      <td>幼年型慢性粒细胞白血病, 子宫内膜间质肉瘤</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alias</td>\n",
       "      <td>8877</td>\n",
       "      <td>梅毒合并艾滋病, 黏液黏稠病</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drug</td>\n",
       "      <td>4625</td>\n",
       "      <td>天麻素胶囊, 丙酸睾酮</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Symptom</td>\n",
       "      <td>5622</td>\n",
       "      <td>心境不良, 耳廓腹侧面局限性囊肿</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Part</td>\n",
       "      <td>82</td>\n",
       "      <td>盆腔, 下肢</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Department</td>\n",
       "      <td>82</td>\n",
       "      <td>消化内科, 成瘾医学科</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Disease</td>\n",
       "      <td>14336</td>\n",
       "      <td>海鱼分枝杆菌感染, 妊娠呕吐</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Total</td>\n",
       "      <td>36825</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     EntityType Numbers               Examples\n",
       "0  Complication    3201  幼年型慢性粒细胞白血病, 子宫内膜间质肉瘤\n",
       "1         Alias    8877         梅毒合并艾滋病, 黏液黏稠病\n",
       "2          Drug    4625            天麻素胶囊, 丙酸睾酮\n",
       "3       Symptom    5622       心境不良, 耳廓腹侧面局限性囊肿\n",
       "4          Part      82                 盆腔, 下肢\n",
       "5    Department      82            消化内科, 成瘾医学科\n",
       "6       Disease   14336         海鱼分枝杆菌感染, 妊娠呕吐\n",
       "7         Total   36825                       "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 AC Tree创建\n",
    "`pyahocorasick`模块的作用是字符串匹配，比如现在有个数据量很大的列表，根据用户输入一句话，从大列表中匹配出字符串"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_actree(self, wordlist):\n",
    "    \"\"\"\n",
    "    Step1: 构造actree，加速过滤\n",
    "    :param wordlist:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    actree = ahocorasick.Automaton()\n",
    "    # 向树中添加单词\n",
    "    for index, word in enumerate(wordlist):\n",
    "        actree.add_word(word, (index, word))\n",
    "    actree.make_automaton()\n",
    "    return actree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def __init__(self):\n",
    "    cur_dir = '/'.join(os.path.abspath(__file__).split('/')[:-1])\n",
    "    # 路径\n",
    "    self.stopwords_path = os.path.join(cur_dir, 'data/stop_words.utf8')\n",
    "    # self.same_words_path = os.path.join(cur_dir, 'DATA/同义词林.txt')\n",
    "    self.stopwords = [w.strip() for w in open(self.stopwords_path, 'r', encoding='utf8') if w.strip()]\n",
    "\n",
    "    data_dir = os.path.join(cur_dir, 'data/')\n",
    "    self.disease_path = data_dir + 'disease_vocab.txt'\n",
    "    self.symptom_path = data_dir + 'symptom_vocab.txt'\n",
    "    self.alias_path = data_dir + 'alias_vocab.txt'\n",
    "    self.complication_path = data_dir + 'complications_vocab.txt'\n",
    "\n",
    "    self.disease_entities = [w.strip() for w in open(self.disease_path, encoding='utf8') if w.strip()]\n",
    "    self.symptom_entities = [w.strip() for w in open(self.symptom_path, encoding='utf8') if w.strip()]\n",
    "    self.alias_entities = [w.strip() for w in open(self.alias_path, encoding='utf8') if w.strip()]\n",
    "    self.complication_entities = [w.strip() for w in open(self.complication_path, encoding='utf8') if w.strip()]\n",
    "\n",
    "    self.region_words = list(set(self.disease_entities + self.alias_entities + self.symptom_entities))\n",
    "\n",
    "    # 构造领域actree\n",
    "    self.disease_tree = self.build_actree(list(set(self.disease_entities)))\n",
    "    self.alias_tree = self.build_actree(list(set(self.alias_entities)))\n",
    "    self.symptom_tree = self.build_actree(list(set(self.symptom_entities)))\n",
    "    self.complication_tree = self.build_actree(list(set(self.complication_entities)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用`self.disease_tree.iter(question)`可进行快速匹配出结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entity_reg(self, question):\n",
    "    \"\"\"\n",
    "    模式匹配, 得到匹配的词和类型。如疾病，疾病别名，并发症，症状\n",
    "    :param question:str\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    self.result = {}\n",
    "\n",
    "    for i in self.disease_tree.iter(question):\n",
    "        word = i[1][1]\n",
    "        if \"Disease\" not in self.result:\n",
    "            self.result[\"Disease\"] = [word]\n",
    "        else:\n",
    "            self.result[\"Disease\"].append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 使用相似度进行实体匹配"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_sim_words(self, question):\n",
    "    \"\"\"\n",
    "    当全匹配失败时，就采用相似度计算找出相似的词\n",
    "    :param question:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    import re\n",
    "    import string\n",
    "    from gensim.models import KeyedVectors\n",
    "\n",
    "    # 使用jieba加载自定义词典\n",
    "    jieba.load_userdict(self.vocab_path)\n",
    "    # 加载词向量\n",
    "    self.model = KeyedVectors.load_word2vec_format(self.word2vec_path, binary=False)\n",
    "\n",
    "    # 去除一些特殊符号\n",
    "    sentence = re.sub(\"[{}]\", re.escape(string.punctuation), question)\n",
    "    sentence = re.sub(\"[，。‘’；：？、！【】]\", \" \", sentence)\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    # Step2: 使用jieba分词\n",
    "    words = [w.strip() for w in jieba.cut(sentence) if w.strip() not in self.stopwords and len(w.strip()) >= 2]\n",
    "\n",
    "    alist = []\n",
    "\n",
    "    # Step3: 对每个词与词库进行相似度得分计算\n",
    "    for word in words:\n",
    "        temp = [self.disease_entities, self.alias_entities, self.symptom_entities, self.complication_entities]\n",
    "        for i in range(len(temp)):\n",
    "            flag = ''\n",
    "            if i == 0:\n",
    "                flag = \"Disease\"\n",
    "            elif i == 1:\n",
    "                flag = \"Alias\"\n",
    "            elif i == 2:\n",
    "                flag = \"Symptom\"\n",
    "            else:\n",
    "                flag = \"Complication\"\n",
    "            # 计算相似度\n",
    "            scores = self.simCal(word, temp[i], flag)\n",
    "            alist.extend(scores)\n",
    "    \n",
    "    # 取分数最高的实体和所属的实体类型\n",
    "    temp1 = sorted(alist, key=lambda k: k[1], reverse=True)\n",
    "    if temp1:\n",
    "        self.result[temp1[0][2]] = [temp1[0][0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 意图识别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "整体思路：\n",
    "1. 利用`TF-IDF`表征文本特征，并构建人工特征（每一类意图常见词在句子中出现的频数）\n",
    "2. 使用朴素贝叶斯模型进行意图识别\n",
    "3. 使用实体信息进行意图的纠正和补充"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.1 特征构建\n",
    "主要包括`TF-IDF`特征和人工特征的构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(self, text, vectorizer):\n",
    "    \"\"\"\n",
    "    提取问题的TF-IDF特征\n",
    "    :param text:\n",
    "    :param vectorizer:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 加载自定义字典\n",
    "    jieba.load_userdict(self.vocab_path)\n",
    "    # 进行分词\n",
    "    words = [w.strip() for w in jieba.cut(text) if w.strip() and w.strip() not in self.stopwords]\n",
    "    sents = [' '.join(words)]\n",
    "    \n",
    "    # 计算TF-IDF特征值\n",
    "    tfidf = vectorizer.transform(sents).toarray()\n",
    "    return tfidf\n",
    "\n",
    "def other_features(self, text):\n",
    "    \"\"\"\n",
    "    提取问题的关键词特征\n",
    "    :param text:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    features = [0] * 7\n",
    "    for d in self.disase_qwds:\n",
    "        if d in text:\n",
    "            features[0] += 1\n",
    "\n",
    "    for s in self.symptom_qwds:\n",
    "        if s in text:\n",
    "            features[1] += 1\n",
    "\n",
    "    for c in self.cureway_qwds:\n",
    "        if c in text:\n",
    "            features[2] += 1\n",
    "\n",
    "    for c in self.check_qwds:\n",
    "        if c in text:\n",
    "            features[3] += 1\n",
    "    for p in self.lasttime_qwds:\n",
    "        if p in text:\n",
    "            features[4] += 1\n",
    "\n",
    "    for r in self.cureprob_qwds:\n",
    "        if r in text:\n",
    "            features[5] += 1\n",
    "\n",
    "    for d in self.belong_qwds:\n",
    "        if d in text:\n",
    "            features[6] += 1\n",
    "\n",
    "    m = max(features)\n",
    "    n = min(features)\n",
    "    normed_features = []\n",
    "    if m == n:\n",
    "        normed_features = features\n",
    "    else:\n",
    "        for i in features:\n",
    "            j = (i - n) / (m - n)\n",
    "            normed_features.append(j)\n",
    "\n",
    "    return np.array(normed_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.2 使用朴素贝叶斯进行意图识别（文本分类） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 意图预测\n",
    "def extractor(self, question):\n",
    "    # TF-IDF特征\n",
    "    tfidf_feature = self.tfidf_features(question, self.tfidf_model)\n",
    "    # 人工特征\n",
    "    other_feature = self.other_features(question)\n",
    "    m = other_feature.shape\n",
    "    other_feature = np.reshape(other_feature, (1, m[0]))\n",
    "    \n",
    "    feature = np.concatenate((tfidf_feature, other_feature), axis=1)\n",
    "    # 进行预测，得到分类类别\n",
    "    predicted = self.model_predict(feature, self.nb_model)\n",
    "    intentions.append(predicted[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2.3 纠正和补充意图\n",
    "- 已知疾病，查询症状：`query_symptom`\n",
    "- 已知疾病或症状，查询治疗方法：`query_cureway`\n",
    "- 已知疾病或症状，查询治疗周期：`query_period`\n",
    "- 已知疾病，查询治愈率：`query_rate`\n",
    "- 已知疾病，查询检查项目：`query_checklist`\n",
    "- 查询科室：`query_department`\n",
    "- 已知症状，查询疾病：`query_disease`\n",
    "- 若没有检测到意图，且已知疾病，则返回疾病的描述：`disease_describe`\n",
    "- 若是疾病和症状同时出现，且出现了查询疾病的特征词，则意图为查询疾病：`query_disease`\n",
    "- 若没有识别出实体或意图则调用其它方法：`QA_matching`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 总结\n",
    "1. 整个意图识别的过程很清晰，通过使用`AC Tree`和词相似度匹配的方法，得到命名实体，然后根据命名实体，采用`TF-IDF`进行特征提取，再利用已经训练好的朴素贝叶斯模型，进行文本分类预测，即意图识别。\n",
    "2. 通过代码的`Debug`可以看到最后得到的意图，例如“乙肝怎么治”，得到的文本分类为`{'Disease': ['乙肝'], 'intentions': ['query_cureway']}`，表示疾病为乙肝，意图是已知疾病或症状，查询治疗方法\n",
    "![](images\\task04\\02.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
