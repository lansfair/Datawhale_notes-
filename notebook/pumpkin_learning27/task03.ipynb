{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task03 详读西瓜书+南瓜书第4章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 决策树基本流程\n",
    "- 概念：基于树结构来进行决策，体现人类在面临决策问题时一种很自然的处理机制\n",
    "- 具备条件：\n",
    "  1. 每个非叶节点表示一个特征属性测试\n",
    "  2. 每个分支代表这个特征属性在某个值域上的输出\n",
    "  3. 每个叶子节点存放一个类别\n",
    "  4. 每个节点包含的样本集合通过属性测试被划分到子节点中，根节点包含样本全集\n",
    "- 基本算法：  \n",
    "  **输入：** 训练集$D=\\{(x_1,y_1),(x_2,y_2),\\cdot, (x_m,y_m)\\}$;  \n",
    "  &emsp;&emsp;&emsp;属性集$A={a_1,a_2,\\cdot,a_d}$  \n",
    "  **过程：** 函数TreeGenerate($D$,$A$)  \n",
    "  (1) 生成结点node  \n",
    "  (2) **if** $D$中样本全属于同一类别$C$ **then**  \n",
    "  (3) &emsp;将node标记为$C$类叶节点; **return**  \n",
    "  (4) **end if**  \n",
    "  (5) **if** $A=\\emptyset$ **OR** $D$中样本在$A$上取值相同 **then**  \n",
    "  (6) &emsp;将node标记为叶结点，其类别标记为$D$中样本数最多的类；**return**  \n",
    "  (7) **end if**  \n",
    "  (8) 从$A$中选择最优化分属性$a_*$;  \n",
    "  (9) **for** $a_*$的每一个值$a_*^v$ **do**  \n",
    "  (10) &emsp;为node生成一个分支；令$D_v$表示$D$中在$a_*$上取值为$a_*^v$的样本子集;  \n",
    "  (11) &emsp; **if** $D_v$为空 **then**  \n",
    "  (12) &emsp;&emsp; 将分支结点标记为叶结点，其类别标记为$D$中样本最多的类; **return**  \n",
    "  (13) &emsp;**else**  \n",
    "  (14) &emsp;&emsp; 以TreeGenerate($D_v$, $A \\backslash \\{ a_* \\}$)为分支结点  \n",
    "  (15) &emsp;**end if**  \n",
    "  (16) **end for**  \n",
    "  **输出：** 以node为根结点的一棵决策树\n",
    "- 决策树构造\n",
    "  1. 当前结点包含的样本全部属于同一类，直接将该结点标记为叶结点，其类别设置该类\n",
    "  2. 当属性集为空，或所有样本在所有属性上取值相同，无法进行划分，将该结点标记为叶结点，其类别设置为其父结点所含样本最多的类别\n",
    "  3. 当前结点包含的样本集合为空，不能划分，将该结点标记为叶结点，其类别设置为其父结点所含样本最多的类别"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 划分选择\n",
    "\n",
    "### 2.1 信息增益\n",
    "- 信息熵：度量样本集合纯度最常用的一种指标\n",
    "- 信息熵定义：  \n",
    "&emsp;&emsp;假定当前样本集合$D$中第$k$类样本所占的比例为$p_k(k=1,2,\\dots,|\\mathcal{Y}|)$，则$D$的信息熵表示为$$\\text{Ent}(D)=-\\sum_{k=1}^{|\\mathcal{Y }| } p_{k} \\log_2 p_k$$\n",
    "$\\text{Ent}(D)$值越小，则$D$的纯度越高。\n",
    "- 信息增益定义：  \n",
    "&emsp;&emsp;假定使用属性$a$对样本集$D$进行划分，产生了$V$个分支节点，$v$表示其中第$v$个分支节点，易知：分支节点包含的样本数越多，表示该分支节点的影响力越大，可以计算出划分后相比原始数据集$D$获得的“信息增益”（information gain）。\n",
    "$$\\text{Gain}(D, \\alpha)=\\text{Ent}(D)-\\sum_{v=1}^{V} \\frac{|D^{v}|}{|D|} \\text{Ent}(D^v)$$\n",
    "信息增益越大，使用属性$a$划分样本集$D$的效果越好。\n",
    "- ID3决策树学习算法是以信息增益为准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 增益率\n",
    "- 作用：用于解决属性信息熵为0，或远高于其他属性的信息熵问题\n",
    "- 定义：$$\\text{Gain_ratio}(D, \\alpha)=\\frac{\\text{Gain}(D, \\alpha)}{\\text{IV}(\\alpha)}$$其中$$\n",
    "\\text{IV}(\\alpha)=-\\sum_{v=1}^V \\frac{|D^v|}{|D|} \\log_2 \\frac{|D^v|}{|D|}$$当$\\alpha$属性的取值越多时，$\\text{IV}(\\alpha)$值越大\n",
    "- C4.5算法是以增益率为准则"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 基尼指数\n",
    "- CART决策树使用“基尼指数”（Gini index）来选择划分属性\n",
    "- 作用：表示从样本集$D$中随机抽取两个样本，其类别标记不一致的概率，因此$\\text{Gini}(D)$越小越好\n",
    "- 定义：$$\\begin{aligned} \\text{Gini}(D) \n",
    "&=\\sum_{k=1}^{|\\mathcal{Y}|} \\sum_{k' \\neq k} p_k p_{k'} \\\\\n",
    "&=1-\\sum_{k=1}^{|\\mathcal{Y}|} p_k^2 \n",
    "\\end{aligned}$$\n",
    "- 属性选择：  \n",
    "使用属性$\\alpha$划分后的基尼指数为：\n",
    "$$\\text { Gini_index }(D, \\alpha)=\\sum_{v=1}^V \\frac{|D^v|}{|D|} \\text{Gini}(D^v)$$故选择基尼指数最小的划分属性。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
