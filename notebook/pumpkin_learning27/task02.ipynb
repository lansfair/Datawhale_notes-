{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task02 详读西瓜书+南瓜书第3章"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 基本形式\n",
    "- 概念：给定由$d$个属性描述的实例$x=(x_1;x_2;\\dots ;x_d)$，其中$x_i$是$x$在第$i$个属性上的取值，线性模型试图学得一个通过属性的线性组合来进行预测的函数，即$f(x)=w_1x_1+w_2x_2+\\cdot+w_dx_d+b$\n",
    "- 一般用向量形式：$f(x)=w^Tx+b$，其中$w=(w_1;w_2;\\cdot;w_d)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 线性回归\n",
    "- 属性值转换：对离散属性，若属性值之间存在“序关系”，则可以将其转化为连续值；若属性值之间不存在“序关系”，则通常将其转化为向量的形式\n",
    "- 确定$w$和$b$\n",
    "  - 目标：最小化$f(x)$与$y$均方误差\n",
    "  - 输入属性只有一个（最小二乘法）：首先计算出每个样本预测值与真实值之间的误差并求和，通过最小化均方误差MSE，使用求偏导等于零的方法计算出拟合直线$y=wx+b$的两个参数$w$和$b$。\n",
    "  - 输入属性有多个（多元线性回归）：可类似于最小二乘法，将数据集$D$表示为一个$m\\times (d+1)$大小的矩阵，再表示为向量形式，最后通过计算$\\hat{w}$偏导，得到最优解。\n",
    "- 简写形式：$y=w^Tx+b$\n",
    "- 广义线性模型：$y=g^{-1}(w^Tx_b)$，其中$g(\\cdot)$时单调可微函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 对数几率回归\n",
    "- 单位阶跃函数：若预测值$z$大于零判为正例，小于零判为反例，预测值为临界值零则可任意判别。\n",
    "- 对数几率函数：$y=\\frac{1}{1+e^{-z}}$\n",
    "- 概念：若将$y$看做样本为正例的概率，$(1-y)$看做样本为反例的概率，则使用线性回归模型的预测结果器逼近真实标记的对数几率\n",
    "- 思路：使用最大似然估计的方法来计算出$w$和$b$两个参数的取值\n",
    "$\\displaystyle \\ln \\frac{p(y=1 | x)}{p(y=0 | x)}=w^T x + b $  \n",
    "$\\because p(y=1|x)=1-p(y=0|x)$  \n",
    "正例：$\\displaystyle p(y=1|x) = \\frac{e^{w^T x + b}}{1 + e^{w^T x + b}}$  \n",
    "负例：$\\displaystyle p(y=0|x) = \\frac{1}{1 + e^{w^T x + b}}$  \n",
    "似然函数：$\\displaystyle \\ell(w, b)=\\sum_{i=1}^m \\ln p(y_i | x_i ; w, b)$，对数变乘为加，即所有样本出现真实值的概率乘积最大。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 线性判别分析\n",
    "\n",
    "- 线性判别分析（LDA）基本思想：将训练样本投影到一条直线上，使得同类的样例尽可能近，不同类的样例尽可能远。对新样本进行分类时，投影到同一条直线上，根据投影点的位置确定新样本的类别。\n",
    "- 具体步骤：\n",
    "  1. 给定数据集$D=\\{(x_i,y_i)\\}_{i=1}^m, y_i \\in \\{0,1\\}$，令$X_i,\\mu_i, \\Sigma_i$分别表示第$i \\in \\{0,1\\}$类示例的集合、均值向量、协方差矩阵。\n",
    "  2. 若将数据投影到直线$w$上，则两类样本的中心在直线上的投影分别为$w^T \\mu_0$和$w^T \\mu_1$；若将所有样本点都投影到直线上，则两类样本的协方差分别为$w^T \\Sigma_0 w$和$w^T \\Sigma_1 w$。\n",
    "  3. 使得各类的协方差之和尽可能小，不同类之间中心的距离尽可能大。\n",
    "  4. 计算类内散度矩阵：\n",
    "  $$\n",
    "\\begin{aligned} S_w &=\\Sigma_0+\\Sigma_1 \\\\ \n",
    "&=\\sum_{x \\in X_0} (x-\\mu_0) (x-\\mu_0)^T+ \\sum_{x \\in X_1}(x-\\mu_1)(x-\\mu_1)^T \\end{aligned}\n",
    "$$\n",
    "  5. 计算类间散度矩阵：\n",
    "  $$S_b=(\\mu_0-\\mu_1)(\\mu_0-\\mu_1)^T$$\n",
    "  6. 计算LDA最大化的目标函数：$$J=\\frac{w^T S_b w}{w^T S_w w}$$\n",
    "  7. $W$的闭式解是$S_w^{-1}S_b$的$N-1$个最大广义特征值所对应的特征向量组成的矩阵\n",
    "- LDA常被视为一种经典的监督降维技术。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 多分类学习\n",
    "- “拆分”策略：将多分类问题拆解为多个二分类问题，训练出多个二分类学习器，最后将多个分类结果进行集成得出结论。\n",
    "- “一对一”（OvO）：给定数据集$D$，假定其中有$N$个真实类别，将这$N$个类别进行两两配对（一个正类/一个反类），从而产生$N(N-1)/2$个二分类学习器，在测试阶段，将新样本提交给所有学习器，得出$N(N-1)$个结果，最终通过投票产生最终的分类结果。\n",
    "- “一对其余”（OvR）：给定数据集$D$，假定其中有$N$个真实类别，每次取出一个类作为正类，剩余的所有类别作为一个新的反类，从而产生$N$个二分类学习器，在测试阶段，得出$N$个结果，若仅有一个学习器预测为正类，则对应的类标作为最终分类结果。\n",
    "- “多对多”（MvM）：给定数据集$D$，假定其中有$N$个真实类别，每次取若干个类作为正类，若干个类作为反类（通过ECOC码给出，编码），若进行了$M$次划分，则生成了$M$个二分类学习器，在测试阶段（解码），得出$M$个结果组成一个新的编码，最终通过将预测编码与每个类别各自的编码进行比较，选择距离最小的类别作为最终分类结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6 类别不平衡问题\n",
    "- 概念：指分类问题中不同类别的训练样本相差悬殊的情况\n",
    "- 常用方法：\n",
    "  1. 对训练样本较多的类别中进行“欠采样”（undersampling），使得正反例数目接近，常见的算法有：EasyEnsemble。\n",
    "  2. 对训练样本较少的类别中进行“过采样”（oversampling），增加较少类的数量，使得正反例数目接近，常见的算法有SMOTE。\n",
    "  3. 直接基于原数据集进行学习，对预测值进行“再缩放”处理。其中“再缩放”也是“代价敏感学习”的基础。\n",
    "$$\\frac{y'}{1-y'}=\\frac{y}{1-y} \\times \\frac{m^{-}}{m^{+}} = \\frac{y}{1-y} \\times \\frac{\\text{cost} (+>-)}{\\text{cost} (->+)}\\quad$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
