{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task10 前向分步算法与梯度提升决策树"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 知识梳理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 前向分步算法\n",
    "- 加法模型：求解以下函数的最优解$(\\beta,\\gamma)$\n",
    "$$\n",
    "\\min _{\\beta_{m}, \\gamma_{m}} \\sum_{i=1}^{N} L\\left(y_{i}, \\sum_{m=1}^{M} \\beta_{m} b\\left(x_{i} ; \\gamma_{m}\\right)\\right)\n",
    "$$\n",
    "- 前向分布算法：\n",
    "  1. 初始化$f_0(x)=0$\n",
    "  2. 对于$m=1,2,\\cdots,M$，其中$M$表示基本分类器的个数  \n",
    "    - 极小化损失函数：$\\displaystyle \n",
    "   \\left(\\beta_{m}, \\gamma_{m}\\right)=\\arg \\min _{\\beta, \\gamma} \\sum_{i=1}^{N} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+\\beta b\\left(x_{i} ; \\gamma\\right)\\right)$，得到参数$\\beta_{m}$与$\\gamma_{m}$  \n",
    "     - 更新：$ f_{m}(x)=f_{m-1}(x)+\\beta_{m} b\\left(x ; \\gamma_{m}\\right)$\n",
    "  3. 得到加法模型：$\\displaystyle f(x)=f_{M}(x)=\\sum_{m=1}^{M} \\beta_{m} b\\left(x ; \\gamma_{m}\\right)$\n",
    "- Adaboost算法是前向分步算法的特例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 梯度提升决策树（GBDT）\n",
    "\n",
    "#### 1.2.1 基于残差学习的提升树算法\n",
    "- 最佳划分点判断：\n",
    "  - 分类树：用**纯度**来判断，使用信息增益（ID3算法），信息增益比（C4.5算法），基尼系数（CART分类树）\n",
    "  - 回归树：用**平方误差**判断\n",
    "- 用**每个样本的残差**修正样本权重以及计算每个基本分类器的权重\n",
    "- 算法步骤：  \n",
    "  1. 初始化$f_0(x) = 0$                        \n",
    "  2. 对$m = 1,2,...,M$：                  \n",
    "    - 计算每个样本的残差:$r_{m i}=y_{i}-f_{m-1}\\left(x_{i}\\right), \\quad i=1,2, \\cdots, N$  \n",
    "    - 拟合残差$r_{mi}$学习一棵回归树，得到$T\\left(x ; \\Theta_{m}\\right)$                        \n",
    "    - 更新$f_{m}(x)=f_{m-1}(x)+T\\left(x ; \\Theta_{m}\\right)$\n",
    "  3. 得到最终的回归问题的提升树：$\\displaystyle f_{M}(x)=\\sum_{m=1}^{M} T\\left(x ; \\Theta_{m}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 梯度提升决策树算法（GBDT）\n",
    "- 算法思路：  \n",
    "  使用最速下降法的近似方法，利用损失函数的负梯度在当前模型的值$-\\left[\\frac{\\partial L\\left(y, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)}$，作为回归问题提升树算法中的残差的近似值，拟合回归树。\n",
    "- 算法步骤：\n",
    "  1. 初始化$f_{0}(x)=\\arg \\min _{c} \\sum_{i=1}^{N} L\\left(y_{i}, c\\right)$                     \n",
    "  2. 对于$m=1,2,\\cdots,M$：                   \n",
    "    - 对$i = 1,2,\\cdots,N$计算：$r_{m i}=-\\left[\\frac{\\partial L\\left(y_{i}, f\\left(x_{i}\\right)\\right)}{\\partial f\\left(x_{i}\\right)}\\right]_{f(x)=f_{m-1}(x)}$                \n",
    "    - 对$r_{mi}$拟合一个回归树，得到第m棵树的叶结点区域$R_{m j}, j=1,2, \\cdots, J$                           \n",
    "    - 对$j=1,2,\\cdots,J$，计算：$c_{m j}=\\arg \\min _{c} \\sum_{x_{i} \\in R_{m j}} L\\left(y_{i}, f_{m-1}\\left(x_{i}\\right)+c\\right)$                      \n",
    "    - 更新$f_{m}(x)=f_{m-1}(x)+\\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right)$                    \n",
    "  3. 得到回归树：$\\hat{f}(x)=f_{M}(x)=\\sum_{m=1}^{M} \\sum_{j=1}^{J} c_{m j} I\\left(x \\in R_{m j}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 实战练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 基于残差学习的提升树算法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.arange(1, 11)\n",
    "y = np.array([5.56, 5.7, 5.91, 6.4, 6.8, 7.05, 8.9, 8.7, 9, 9.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tree:\n",
    "    def __init__(self, split_point, mse, left_value, right_value, residual):\n",
    "        # feature最佳切分点\n",
    "        self.best_split_point = split_point\n",
    "        # 平方误差\n",
    "        self.mse = mse\n",
    "        # 左子树值\n",
    "        self.left_value = left_value\n",
    "        # 右子树值\n",
    "        self.right_value = right_value\n",
    "        # 每棵决策树生成后的残差\n",
    "        self.residual = residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBT:\n",
    "    def __init__(self, X, y, tol=0.05, n_estimators=6):\n",
    "        # 训练数据：实例\n",
    "        self.X = X\n",
    "        # 训练数据：标签\n",
    "        self.y = y\n",
    "        # 最大迭代次数\n",
    "        self.n_estimators = n_estimators\n",
    "        # 回归树\n",
    "        self.T = []\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        对训练数据进行学习\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        # 得到切分点\n",
    "        split_point = self.split_point()\n",
    "\n",
    "        residual = self.y.copy()\n",
    "        for i in range(self.n_estimators):\n",
    "            tree, residual = self.build_desicion_tree(split_point, residual)\n",
    "            self.T.append(tree)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        对新数据进行预测\n",
    "        \"\"\"\n",
    "        m = np.shape(X)[0]\n",
    "        y_predict = np.zeros(m)\n",
    "\n",
    "        for tree in self.T:\n",
    "            for i in range(m):\n",
    "                if X[i] < tree.best_split_point:\n",
    "                    y_predict[i] += tree.left_value\n",
    "                else:\n",
    "                    y_predict[i] += tree.right_value\n",
    "        return y_predict\n",
    "\n",
    "    def sse(self):\n",
    "        \"\"\"平方损失误差\"\"\"\n",
    "        y_predict = self.predict(X)\n",
    "        return np.sum((y_predict - y) ** 2)\n",
    "\n",
    "    def score(self, X, y):\n",
    "        \"\"\"对训练效果进行评价\"\"\"\n",
    "        y_predict = self.predict(X)\n",
    "        error_rate = np.sum(np.abs(y_predict - y)) / 2 / y.shape[0]\n",
    "        return 1 - error_rate\n",
    "\n",
    "    def split_point(self):\n",
    "        \"\"\"\n",
    "        获取切分点\n",
    "        :return: 切分点\n",
    "        \"\"\"\n",
    "        return (self.X[0:-1] + self.X[1:]) / 2\n",
    "\n",
    "    def build_desicion_tree(self, split_point, label):\n",
    "        m_s_list = []\n",
    "        c1_list = []\n",
    "        c2_list = []\n",
    "        for p in split_point:\n",
    "            # 切分点左边的label值\n",
    "            label_left = label[0:int(p)]\n",
    "            # 切分点右边的label值\n",
    "            label_right = label[int(p):]\n",
    "            c1 = np.mean(label_left)\n",
    "            c2 = np.mean(label_right)\n",
    "            m_s = np.sum((label_left - c1) ** 2) + np.sum((label_right - c2) ** 2)\n",
    "            c1_list.append(c1)\n",
    "            c2_list.append(c2)\n",
    "            m_s_list.append(m_s)\n",
    "        # 得到m_s最小值所在的位置\n",
    "        best_index = np.argmin(m_s_list)\n",
    "        # 得到最优切分点\n",
    "        best_split_point = split_point[int(best_index)]\n",
    "        # 得到最优均方误差\n",
    "        best_mse = m_s_list[int(best_index)]\n",
    "        # 得到左子树的label值\n",
    "        best_y_lf = label[0:int(best_split_point)]\n",
    "        lf_value = np.mean(best_y_lf)\n",
    "        # 得到右子树的label值\n",
    "        best_y_rt = label[int(best_split_point):]\n",
    "        rt_value = np.mean(best_y_rt)\n",
    "        # 得到决策树的残差\n",
    "        residual = np.concatenate((best_y_lf - lf_value, best_y_rt - rt_value,))\n",
    "        tree = Tree(best_split_point, best_mse, lf_value, rt_value, residual)\n",
    "        return tree, residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "原始输出: [5.56 5.7  5.91 6.4  6.8  7.05 8.9  8.7  9.   9.05]\n",
      "预测输出: [5.63       5.63       5.81831019 6.55164352 6.81969907 6.81969907\n",
      " 8.95016204 8.95016204 8.95016204 8.95016204]\n",
      "预测正确率：94.58%\n",
      "平方损失误差：0.17\n"
     ]
    }
   ],
   "source": [
    "clf = ResidualBT(X, y, n_estimators=6)\n",
    "clf.fit()\n",
    "y_predict = clf.predict(X)\n",
    "score = clf.score(X, y)\n",
    "print(\"\\n原始输出:\", y)\n",
    "print(\"预测输出:\", y_predict)\n",
    "print(\"预测正确率：{:.2%}\".format(score))\n",
    "print(\"平方损失误差：{:.2}\".format(clf.sse()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 梯度提升决策树算法（GBDT）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = [[5, 20, 1.1],\n",
    "     [7, 30, 1.3],\n",
    "     [21, 70, 1.7],\n",
    "     [30, 60, 1.8]]\n",
    "df = pd.DataFrame(columns=['age', 'weight', 'height'], data=data)\n",
    "X = df[['age', 'weight']].values\n",
    "y = df['height'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.56713975"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# 学习率：learning_rate=0.1，迭代次数：n_trees=5，树的深度：max_depth=3\n",
    "gbr = GradientBoostingRegressor(n_estimators=5, learning_rate=0.1,\n",
    "    max_depth=3, random_state=0, loss='ls')\n",
    "gbr.fit(X, y)\n",
    "gbr.predict([[25, 65]])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 练一练\n",
    "GradientBoostingRegressor与GradientBoostingClassifier函数各个参数的意思"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 GradientBoostingRegressor各个参数的意思\n",
    "\n",
    "- loss：{`ls`,`lad`,`huber`,`quantile`}, default=`ls`：  \n",
    "  - `ls`：指最小二乘回归；   \n",
    "  - `lad`：(最小绝对偏差) 是仅基于输入变量的顺序信息，具有高度鲁棒的损失函数；  \n",
    "  - `huber`：上述两者的结合；  \n",
    "  - `quantile`：允许分位数回归（用于alpha指定分位数）  \n",
    "- learning_rate：学习率用于缩小每棵树的贡献learning_rate，在learning_rate和n_estimators之间需要权衡\n",
    "- n_estimators：执行迭代次数\n",
    "- subsample：用于拟合各个基学习器的样本比例。如果小于1.0，将使得随机梯度增强。subsample与参数n_estimators有关联，选择subsample<1.0会导致方差减少和偏差增加\n",
    "- criterion：{`friedman_mse`，`mse`，`mae`}，默认为`friedman_mse`：`mse`是均方误差，`mae`是平均绝对误差。默认值`friedman_mse`通常是最好的，因为在大多情况下可以提供更好的近似值\n",
    "- min_samples_split：默认为2，拆分内部节点所需的最少样本数\n",
    "- min_samples_leaf：默认为1，在叶节点处需要的最小样本数\n",
    "- min_weight_fraction_leaf：默认为0.0，在所有叶节点处（所有输入样本）的权重总和中的最小加权数。如果未提供sample_weight，则样本的权重相等\n",
    "- max_depth：默认为3，各个回归模型的最大深度。最大深度限制了树中节点的数量。调整此参数以获得最佳性能；最佳值取决于输入变量\n",
    "- min_impurity_decrease：如果节点拆分会导致不纯度大于或等于该值，则该节点将被拆分。\n",
    "- min_impurity_split：提前停止树生长的阈值。如果节点的不纯度高于该值，则该节点将拆分\n",
    "- max_features {`auto`, `sqrt`, `log2`}，int或float：寻找最佳切分点时要考虑的特征个数：\n",
    "  - 如果是int，则表示节点切分的特征个数\n",
    "  - 如果是float，max_features则为小数，根据公式int(max_features * n_features)确定节点切分的特征个数\n",
    "  - 如果是`auto`，则max_features=n_features\n",
    "  - 如果是`sqrt`，则max_features=sqrt(n_features)\n",
    "  - 如果为`log2`，则为max_features=log2(n_features)\n",
    "  - 如果没有，则max_features=n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 GradientBoostingClassifier各个参数的意思"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- loss：{`deviance`,`exponential`}, default=`deviance`：\n",
    "  - `deviance`是指对具有概率输出的分类（等同于logistic回归）\n",
    "  - 对于`exponential`梯度提升方法，可等同于AdaBoost算法\n",
    "- learning_rate：学习率用于缩小每棵树的贡献learning_rate，在learning_rate和n_estimators之间需要权衡\n",
    "- n_estimators：执行迭代次数\n",
    "- subsample：用于拟合各个基学习器的样本比例。如果小于1.0，将使得随机梯度增强。subsample与参数n_estimators有关联，选择subsample<1.0会导致方差减少和偏差增加\n",
    "- criterion：{`friedman_mse`，`mse`，`mae`}，默认为`friedman_mse`：`mse`是均方误差，`mae`是平均绝对误差。默认值`friedman_mse`通常是最好的，因为在大多情况下可以提供更好的近似值\n",
    "- min_samples_split：默认为2，拆分内部节点所需的最少样本数\n",
    "- min_samples_leaf：默认为1，在叶节点处需要的最小样本数\n",
    "- min_weight_fraction_leaf：默认为0.0，在所有叶节点处（所有输入样本）的权重总和中的最小加权数。如果未提供sample_weight，则样本的权重相等\n",
    "- max_depth：默认为3，各个回归模型的最大深度。最大深度限制了树中节点的数量。调整此参数以获得最佳性能；最佳值取决于输入变量\n",
    "- min_impurity_decrease：如果节点拆分会导致不纯度大于或等于该值，则该节点将被拆分。\n",
    "- min_impurity_split：提前停止树生长的阈值。如果节点的不纯度高于该值，则该节点将拆分\n",
    "- max_features {`auto`, `sqrt`, `log2`}，int或float：寻找最佳切分点时要考虑的特征个数：\n",
    "  - 如果是int，则表示节点切分的特征个数\n",
    "  - 如果是float，max_features则为小数，根据公式int(max_features * n_features)确定节点切分的特征个数\n",
    "  - 如果是`auto`，则max_features=n_features\n",
    "  - 如果是`sqrt`，则max_features=sqrt(n_features)\n",
    "  - 如果为`log2`，则为max_features=log2(n_features)\n",
    "  - 如果没有，则max_features=n_features"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
