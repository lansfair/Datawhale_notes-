{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Task05 超大图上的节点表征学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 知识梳理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Cluster-GCN方法\n",
    "- 基本思路：利用图节点聚类算法，将一个图的节点划分为多个簇，每一次选择几个簇的节点和这些节点对应边，构成一个子图，然后对子图训练\n",
    "- 效果：\n",
    "    1. 提高表征利用率，提高图神经网络的训练效率\n",
    "    2. 随机选择多个簇组成batch，保证batch内类别分布\n",
    "    3. 内存空间的优化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 节点表征学习\n",
    "- 节点表征递推公式：$\\displaystyle Z^{(l+1)}=A^{\\prime} X^{(l)} W^{(l)}, X^{(l+1)}=\\sigma\\left(Z^{(l+1)}\\right)$\n",
    "- 训练目标：最小化损失函数来学习权重矩阵$\\displaystyle \\mathcal{L}=\\frac{1}{\\left|\\mathcal{Y}_{L}\\right|} \\sum_{i \\in \\mathcal{Y}_{L}} \\operatorname{loss}\\left(y_{i}, z_{i}^{L}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cluster-GCN方法详细分析\n",
    "- 内存消耗：存储所有的节点表征矩阵，需要$O(NFL)$空间\n",
    "- mini-batch SGD方式训练：不需要计算完整梯度，只需要计算部分梯度\n",
    "- 训练时间：一个节点梯度计算需要$O(d^L F^2)$时间\n",
    "- 节点表征的利用率：在训练过程中，如果节点$i$在$l$层的表征$z_{i}^{(l)}$被计算并在$l+1$层的表征计算中被重复使用$u$次"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 简单的Cluster-GCN方法\n",
    "- 基本步骤：\n",
    "  1. 将节点划分为$c$个簇，邻接矩阵被划分为大小为$c^2$的块矩阵\n",
    "  2. 用块对角线邻接矩阵$\\bar{A}$去近似邻接矩阵$A$\n",
    "  3. 采样一个簇$\\mathcal{V}_{t}$，根据$\\mathcal{L}_{{\\bar{A}^{\\prime}}_{tt}}$的梯度进行参数更新\n",
    "- 时间复杂度：\n",
    "  1. 每个batch的总体时间复杂度为$O\\left(\\left\\|A_{t t}\\right\\|_{0} F+ b F^{2}\\right)$\n",
    "  2. 每个epoch的总体时间复杂度为$O\\left(\\|A\\|_{0} F+N F^{2}\\right)$\n",
    "  3. 总时间复杂度为$O\\left(L\\|A\\|_{0} F+LN F^{2}\\right)$\n",
    "- 表征数：每个batch只需要计算$O(b L)$的表征\n",
    "- 空间复杂度：用于存储表征的内存为$O(bLF)$，总空间复杂度为$O(bLF + LF^2)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 实战练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "类别数： 41\n",
      "节点数： 232965\n",
      "边数： 114615892\n",
      "节点维度： 602\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Reddit\n",
    "\n",
    "dataset = Reddit('dataset/Reddit')\n",
    "data = dataset[0]\n",
    "print(\"类别数：\", dataset.num_classes)\n",
    "print(\"节点数：\", data.num_nodes)\n",
    "print(\"边数：\", data.num_edges)\n",
    "print(\"节点维度：\", data.num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import ClusterData, ClusterLoader, NeighborSampler\n",
    "\n",
    "# 图节点聚类\n",
    "cluster_data = ClusterData(\n",
    "    data, num_parts=1500, recursive=False, save_dir=dataset.processed_dir)\n",
    "train_loader = ClusterLoader(\n",
    "    cluster_data, batch_size=20, shuffle=True, num_workers=12)\n",
    "subgraph_loader = NeighborSampler(\n",
    "    data.edge_index, sizes=[-1], batch_size=1024, shuffle=False, num_workers=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import ModuleList\n",
    "from torch.nn import functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from tqdm import tqdm\n",
    "\n",
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(Net, self).__init__()\n",
    "        self.convs = ModuleList(\n",
    "            [SAGEConv(in_channels, 128),\n",
    "             SAGEConv(128, out_channels)])\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            if i != len(self.convs) - 1:\n",
    "                x = F.relu(x)\n",
    "                x = F.dropout(x, p=0.5, training=self.training)\n",
    "        return F.log_softmax(x, dim=-1)\n",
    "\n",
    "    def inference(self, x_all):\n",
    "        pbar = tqdm(total=x_all.size(0) * len(self.convs))\n",
    "        pbar.set_description('Evaluating')\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            xs = []\n",
    "            for batch_size, n_id, adj in subgraph_loader:\n",
    "                edge_index, _, size = adj.to(device)\n",
    "                x = x_all[n_id].to(device)\n",
    "                x_target = x[:size[1]]\n",
    "                x = conv((x, x_target), edge_index)\n",
    "                if i != len(self.convs) - 1:\n",
    "                    x = F.relu(x)\n",
    "                xs.append(x.cpu())\n",
    "\n",
    "                pbar.update(batch_size)\n",
    "\n",
    "            x_all = torch.cat(xs, dim=0)\n",
    "\n",
    "        pbar.close()\n",
    "\n",
    "        return x_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    total_loss = total_nodes = 0\n",
    "    for batch in train_loader:\n",
    "        batch = batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        out = model(batch.x, batch.edge_index)\n",
    "        loss = F.nll_loss(out[batch.train_mask], batch.y[batch.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        nodes = batch.train_mask.sum().item()\n",
    "        total_loss += loss.item() * nodes\n",
    "        total_nodes += nodes\n",
    "\n",
    "    return total_loss / total_nodes\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "\n",
    "    out = model.inference(data.x)\n",
    "    y_pred = out.argmax(dim=-1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = y_pred[mask].eq(data.y[mask]).sum().item()\n",
    "        accs.append(correct / mask.sum().item())\n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01, Loss: 1.1573\n",
      "Epoch: 02, Loss: 0.4642\n",
      "Epoch: 03, Loss: 0.3904\n",
      "Epoch: 04, Loss: 0.3572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 465930/465930 [01:05<00:00, 7123.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 05, Loss: 0.3385, Train: 0.9579, Val: 0.9535, test: 0.9520\n",
      "Epoch: 06, Loss: 0.3139\n",
      "Epoch: 07, Loss: 0.3060\n",
      "Epoch: 08, Loss: 0.2950\n",
      "Epoch: 09, Loss: 0.2933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 465930/465930 [01:02<00:00, 7484.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Loss: 0.2879, Train: 0.9609, Val: 0.9528, test: 0.9509\n",
      "Epoch: 11, Loss: 0.2902\n",
      "Epoch: 12, Loss: 0.2871\n",
      "Epoch: 13, Loss: 0.2803\n",
      "Epoch: 14, Loss: 0.3023\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 465930/465930 [01:01<00:00, 7548.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Loss: 0.2796, Train: 0.9571, Val: 0.9465, test: 0.9436\n",
      "Epoch: 16, Loss: 0.2711\n",
      "Epoch: 17, Loss: 0.2619\n",
      "Epoch: 18, Loss: 0.2726\n",
      "Epoch: 19, Loss: 0.2710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 465930/465930 [01:02<00:00, 7476.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20, Loss: 0.2538, Train: 0.9663, Val: 0.9501, test: 0.9499\n",
      "Epoch: 21, Loss: 0.2532\n",
      "Epoch: 22, Loss: 0.2457\n",
      "Epoch: 23, Loss: 0.2470\n",
      "Epoch: 24, Loss: 0.2340\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 465930/465930 [01:03<00:00, 7384.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 25, Loss: 0.2428, Train: 0.9666, Val: 0.9503, test: 0.9491\n",
      "Epoch: 26, Loss: 0.2469\n",
      "Epoch: 27, Loss: 0.2340\n",
      "Epoch: 28, Loss: 0.2511\n",
      "Epoch: 29, Loss: 0.2444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 465930/465930 [01:02<00:00, 7445.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 30, Loss: 0.2278, Train: 0.9684, Val: 0.9516, test: 0.9507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Net(dataset.num_features, dataset.num_classes).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n",
    "\n",
    "for epoch in range(1, 31):\n",
    "    loss = train()\n",
    "    if epoch % 5 == 0:\n",
    "        train_acc, val_acc, test_acc = test()\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}, Train: {train_acc:.4f}, '\n",
    "              f'Val: {val_acc:.4f}, test: {test_acc:.4f}')\n",
    "    else:\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
